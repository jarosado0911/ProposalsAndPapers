\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{nsf}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Runtimes of simulations of a \textbf  {3d Laplace problem} on a fixed unit cube domain (seven regular refinements) with uG4\xspace  on the SDSC Comet HPC cluster. Note that each node can allocate at maximum 24 processes and the maximum number of processes or cores per job is limited to 1728 processes in total.\relax }}{1}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:xsede_strong_scaling}{{1}{1}{Runtimes of simulations of a \textbf {3d Laplace problem} on a fixed unit cube domain (seven regular refinements) with \ug on the SDSC Comet HPC cluster. Note that each node can allocate at maximum 24 processes and the maximum number of processes or cores per job is limited to 1728 processes in total.\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet.\relax }}{2}{figure.caption.3}}
\newlabel{fig:speedup_laplace}{{1}{2}{Speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet.\relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Runtimes of simulations of a \textbf  {3d Laplace problem} on a unit cube domain (different number of regular refinements) with uG4\xspace  on the SDSC Comet HPC cluster. Note that each grid refinement increases the DoFs by a factor 8.\relax }}{2}{table.caption.4}}
\newlabel{tab:xsede_weak_scaling}{{2}{2}{Runtimes of simulations of a \textbf {3d Laplace problem} on a unit cube domain (different number of regular refinements) with \ug on the SDSC Comet HPC cluster. Note that each grid refinement increases the DoFs by a factor 8.\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Runtimes of simulations of a \textbf  {3d Laplace problem} on a unit cube domain (different number of regular refinements) with uG4\xspace  on the TACC Stampede2 HPC cluster. Note that each grid refinement increases the DoFs by a factor of 8. Note that the \textbf  {large} queue on Stampede2 is not yet available to us to benchmark, thus we are restricted to process count up to 32768.\relax }}{3}{table.caption.6}}
\newlabel{fig:xsede_weak_scaling}{{3}{3}{Runtimes of simulations of a \textbf {3d Laplace problem} on a unit cube domain (different number of regular refinements) with \ug on the TACC Stampede2 HPC cluster. Note that each grid refinement increases the DoFs by a factor of 8. Note that the \textbf {large} queue on Stampede2 is not yet available to us to benchmark, thus we are restricted to process count up to 32768.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Scaled speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet. Note the optimal scaling and achieved scaling behaviour of the test problem.\relax }}{3}{figure.caption.5}}
\newlabel{fig:scaled_speedup_laplace}{{2}{3}{Scaled speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet. Note the optimal scaling and achieved scaling behaviour of the test problem.\relax }{figure.caption.5}{}}
\newlabel{eq:ccyt}{{3}{3}{Performance and Scaling for \ug }{equation.0.3}{}}
\newlabel{eq:buff}{{4}{3}{Performance and Scaling for \ug }{equation.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Scaled speedup of distributed execution. \#procs denotes the involved processes on the TACC system Stampede2. Note the optimal scaling and achieved scaling behaviour of the test problem.\relax }}{4}{figure.caption.7}}
\newlabel{fig:scaled_speedup_laplace2}{{3}{4}{Scaled speedup of distributed execution. \#procs denotes the involved processes on the TACC system Stampede2. Note the optimal scaling and achieved scaling behaviour of the test problem.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Runtime of a simulations on a \textbf  {spine} reconstruction in three-dimensional space. Note the grid has been regularly refined two times with uG4\xspace  and the DoFs increase by a factor 8 and thus also the processes have to increase eightfold. Runtime cost increases slightly but remains bound.\relax }}{4}{table.caption.8}}
\newlabel{tab:spine_speedup}{{4}{4}{Runtime of a simulations on a \textbf {spine} reconstruction in three-dimensional space. Note the grid has been regularly refined two times with \ug and the DoFs increase by a factor 8 and thus also the processes have to increase eightfold. Runtime cost increases slightly but remains bound.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Scaled speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet.\relax }}{5}{figure.caption.9}}
\newlabel{fig:scaled_speedup_spine}{{4}{5}{Scaled speedup of distributed execution. \#procs denotes the involved processes on the SDSC system Comet.\relax }{figure.caption.9}{}}
